{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Configurar la variable ANTES de cualquier import de TF/Keras\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tf_keras # Importante para confirmar instalación\n",
        "\n",
        "# Verificación de Diagnóstico\n",
        "import keras\n",
        "print(f\"Versión de Keras cargada: {keras.__version__}\")\n",
        "\n",
        "if not keras.__version__.startswith(\"2\"):\n",
        "    print(\"Sigue en Keras 3. Reinicia el entorno nuevamente.\")\n",
        "else:\n",
        "    print(\" Keras 2 (Legacy). El código de TF Hub funcionará.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQg6WoGU4jHR",
        "outputId": "4a40afed-c2b5-40ba-c98a-3f423f0c48a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de Keras cargada: 3.10.0\n",
            "❌ ERROR CRÍTICO: Sigues en Keras 3. Reinicia el entorno nuevamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knWhgvaeduzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee902cc-0b5b-4e1e-e822-4a2fcc0f214f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting isic-cli\n",
            "  Downloading isic_cli-12.4.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: click>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from isic-cli) (8.3.1)\n",
            "Collecting django-s3-file-field-client>=1.0.0 (from isic-cli)\n",
            "  Downloading django_s3_file_field_client-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting girder-cli-oauth-client<1.0.0 (from isic-cli)\n",
            "  Downloading girder_cli_oauth_client-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from isic-cli) (4.14.0)\n",
            "Collecting isic-metadata>=1.2.0 (from isic-cli)\n",
            "  Downloading isic_metadata-4.11.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from isic-cli) (10.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from isic-cli) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from isic-cli) (2.32.4)\n",
            "Collecting retryable-requests (from isic-cli)\n",
            "  Downloading retryable_requests-0.1.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from isic-cli) (13.9.4)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.12/dist-packages (from isic-cli) (2.45.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (from isic-cli) (8.5.0)\n",
            "Requirement already satisfied: authlib in /usr/local/lib/python3.12/dist-packages (from girder-cli-oauth-client<1.0.0->isic-cli) (1.6.5)\n",
            "Collecting pyxdg (from girder-cli-oauth-client<1.0.0->isic-cli)\n",
            "  Downloading pyxdg-0.28-py2.py3-none-any.whl.metadata (567 bytes)\n",
            "Requirement already satisfied: pydantic>=2.4 in /usr/local/lib/python3.12/dist-packages (from isic-metadata>=1.2.0->isic-cli) (2.11.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->isic-cli) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->isic-cli) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->isic-cli) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->isic-cli) (2025.11.12)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from retryable-requests->isic-cli) (1.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->isic-cli) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->isic-cli) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->isic-cli) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.4->isic-metadata>=1.2.0->isic-cli) (0.4.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib->girder-cli-oauth-client<1.0.0->isic-cli) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib->girder-cli-oauth-client<1.0.0->isic-cli) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib->girder-cli-oauth-client<1.0.0->isic-cli) (2.23)\n",
            "Downloading isic_cli-12.4.0-py3-none-any.whl (34 kB)\n",
            "Downloading django_s3_file_field_client-1.1.0-py3-none-any.whl (3.2 kB)\n",
            "Downloading girder_cli_oauth_client-0.4.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading isic_metadata-4.11.0-py3-none-any.whl (27 kB)\n",
            "Downloading retryable_requests-0.1.2-py3-none-any.whl (7.5 kB)\n",
            "Downloading pyxdg-0.28-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyxdg, django-s3-file-field-client, retryable-requests, isic-metadata, girder-cli-oauth-client, isic-cli\n",
            "Successfully installed django-s3-file-field-client-1.1.0 girder-cli-oauth-client-0.4.0 isic-cli-12.4.0 isic-metadata-4.11.0 pyxdg-0.28 retryable-requests-0.1.2\n"
          ]
        }
      ],
      "source": [
        "# Instala la herramienta Consola del archivo ISIC\n",
        "!pip install isic-cli #Sacada desde el repositorio de Github https://github.com/ImageMarkup/isic-cli/blob/master/README.md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargar Imagenes + Metadatos\n",
        "!isic image download --search \"\" --collections \"249\" --limit 0 BCN20000/ # Imagenes y Metadatos de BCN20000\n",
        "\n",
        "#Comando extraido desde la misma ISIC ARCHIVE https://api.isic-archive.com/images/?query=&collections=249"
      ],
      "metadata": {
        "id": "CS5EkWzRdzij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129ba994-92c0-4b32-9341-bbfa33ad796b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you have been granted special permissions, logging in with `isic user login` might return more data.\n",
            "\n",
            "\u001b[2KDownloading images + metadata (18,946 files, 1.2 GB) \u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "\u001b[32mSuccessfully downloaded 18,946 images to BCN20000/.\u001b[0m\n",
            "\u001b[32mSuccessfully wrote 18,946 metadata records to BCN20000/metadata.csv.\u001b[0m\n",
            "\u001b[32mSuccessfully wrote attributions to BCN20000/attribution.txt.\u001b[0m\n",
            "\u001b[32mSuccessfully wrote 1 license(s) to BCN20000/licenses.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def cargar_metadatos(archivo):\n",
        "\n",
        "    datos = {\n",
        "        'isic_id': [],\n",
        "        'diagnosis_1': []  # Las etiquetas serán 0 o 1\n",
        "    }\n",
        "    etiquetas_validas = {'Benign': 0, 'Malignant': 1}  # Mapeamos las etiquetas a 0 y 1\n",
        "\n",
        "    try:\n",
        "        with open(archivo, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader)  # Evita las columnas cabeceras.\n",
        "\n",
        "            diagnosis_idx =7  # Cambiar al índice correspondiente si es necesario\n",
        "            #7 para BCN20000\n",
        "            for fila in reader:\n",
        "                if len(fila) > diagnosis_idx:\n",
        "                    diagnosis = fila[diagnosis_idx]\n",
        "                    if diagnosis in etiquetas_validas:  # Solo almacenar Benign y Malignant\n",
        "                        datos['isic_id'].append(fila[0])  # Almacena el ID de la imagen\n",
        "                        datos['diagnosis_1'].append(etiquetas_validas[diagnosis])  # Asignamos la etiqueta 0 o 1\n",
        "\n",
        "        print(f\"Datos extraídos y filtrados correctamente. Se encontraron {len(datos['isic_id'])} registros válidos.\")\n",
        "        return datos\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archivo {archivo} no encontrado\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar el archivo: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "kITLHOIpSi5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "def organizar_imagenes_en_carpetas(carpeta_origen, metadatos, carpeta_destino_base):\n",
        "\n",
        "    # Crear las carpetas de destino\n",
        "    for split in ['train', 'test']:\n",
        "        for class_name in ['benign', 'malignant']:\n",
        "            os.makedirs(os.path.join(carpeta_destino_base, split, class_name), exist_ok=True)\n",
        "\n",
        "    # Separar los IDs por clase\n",
        "    ids_benignos = []\n",
        "    ids_malignos = []\n",
        "    for id_img, etiqueta in zip(metadatos['isic_id'], metadatos['diagnosis_1']):\n",
        "        if etiqueta == 0:  # Benign\n",
        "            ids_benignos.append(id_img)\n",
        "        elif etiqueta == 1: # Malignant\n",
        "            ids_malignos.append(id_img)\n",
        "\n",
        "    # Divide los IDs en conjuntos de entrenamiento y prueba para cada clase\n",
        "    ids_benignos_train, ids_benignos_test = train_test_split(ids_benignos, test_size=0.3, random_state=42)\n",
        "    ids_malignos_train, ids_malignos_test = train_test_split(ids_malignos, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Función auxiliar para copiar archivos\n",
        "    def copiar_archivos(ids, carpeta_destino):\n",
        "        for id_img in tqdm(ids, desc=f'Copiando a {os.path.basename(carpeta_destino)}'):\n",
        "            nombre_archivo = f\"{id_img}.jpg\"\n",
        "            ruta_origen = os.path.join(carpeta_origen, nombre_archivo)\n",
        "            ruta_destino = os.path.join(carpeta_destino, nombre_archivo)\n",
        "            if os.path.exists(ruta_origen):\n",
        "                shutil.copy(ruta_origen, ruta_destino)\n",
        "            else:\n",
        "                print(f\"Advertencia: No se encontró el archivo {ruta_origen}\")\n",
        "\n",
        "    # Copiar los archivos a sus directorios finales\n",
        "    print(\"Copiando imágenes de entrenamiento benignas...\")\n",
        "    copiar_archivos(ids_benignos_train, os.path.join(carpeta_destino_base, 'train', 'benign'))\n",
        "\n",
        "    print(\"Copiando imágenes de entrenamiento malignas...\")\n",
        "    copiar_archivos(ids_malignos_train, os.path.join(carpeta_destino_base, 'train', 'malignant'))\n",
        "\n",
        "    print(\"Copiando imágenes de prueba benignas...\")\n",
        "    copiar_archivos(ids_benignos_test, os.path.join(carpeta_destino_base, 'test', 'benign'))\n",
        "\n",
        "    print(\"Copiando imágenes de prueba malignas...\")\n",
        "    copiar_archivos(ids_malignos_test, os.path.join(carpeta_destino_base, 'test', 'malignant'))\n",
        "\n",
        "    print(\"\\nImágenes organizadas correctamente en carpetas 'train' y 'test' sin fugas de datos.\")"
      ],
      "metadata": {
        "id": "w1CklMUtSmQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "def build_googlenet_benchmark_legacy(num_classes=2):\n",
        "    # URL oficial de GoogleNet (Inception v1)\n",
        "    module_url = \"https://tfhub.dev/google/imagenet/inception_v1/feature_vector/5\"\n",
        "\n",
        "    input_tensor = Input(shape=(224, 224, 3), name='input_image')\n",
        "\n",
        "    base_layer = hub.KerasLayer(module_url, trainable=False, name='inception_v1_backbone')\n",
        "\n",
        "    x = base_layer(input_tensor)\n",
        "\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    output_tensor = layers.Dense(num_classes, activation='softmax', name='classifier_output')(x)\n",
        "\n",
        "    model = models.Model(inputs=input_tensor, outputs=output_tensor, name='GoogleNet_Benchmark_Legacy')\n",
        "\n",
        "    return model\n",
        "\n",
        "model_v1 = build_googlenet_benchmark_legacy()\n",
        "model_v1.summary()"
      ],
      "metadata": {
        "id": "2LKtdWPrSojQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57088d7-0c9c-4d8c-c103-3b407f94c7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"GoogleNet_Benchmark_Legacy\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_image (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " inception_v1_backbone (Ker  (None, 1024)              5607184   \n",
            " asLayer)                                                        \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " classifier_output (Dense)   (None, 2)                 2050      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5609234 (21.40 MB)\n",
            "Trainable params: 2050 (8.01 KB)\n",
            "Non-trainable params: 5607184 (21.39 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Eliminar el directorio del dataset antiguo para evitar conflictos y asegurar una regeneración limpia\n",
        "if os.path.exists('/content/dataset'):\n",
        "    shutil.rmtree('/content/dataset')\n",
        "    print(\"Directorio '/content/dataset' anterior eliminado.\")\n",
        "else:\n",
        "    print(\"El directorio '/content/dataset' no existía, no fue necesaria ninguna limpieza.\")"
      ],
      "metadata": {
        "id": "0QR5PjDQSvEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72eb580-bd66-41cc-e292-5264734eb9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El directorio '/content/dataset' no existía, no fue necesaria ninguna limpieza.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asumiendo que los metadatos ya han sido cargados correctamente\n",
        "metadatos = cargar_metadatos(\"/content/BCN20000/metadata.csv\")"
      ],
      "metadata": {
        "id": "lHX67-w6SyXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65515e5-536c-420b-b7fc-35360eac0d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos extraídos y filtrados correctamente. Se encontraron 16702 registros válidos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "organizar_imagenes_en_carpetas('/content/BCN20000/', metadatos, '/content/dataset')"
      ],
      "metadata": {
        "id": "REGHjnAbSzkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102a889f-6dcd-4966-9b80-756203a482dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copiando imágenes de entrenamiento benignas...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copiando a benign: 100%|██████████| 5481/5481 [00:00<00:00, 5920.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copiando imágenes de entrenamiento malignas...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copiando a malignant: 100%|██████████| 6209/6209 [00:03<00:00, 1580.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copiando imágenes de prueba benignas...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copiando a benign: 100%|██████████| 2350/2350 [00:00<00:00, 3883.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copiando imágenes de prueba malignas...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copiando a malignant: 100%|██████████| 2662/2662 [00:02<00:00, 1080.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imágenes organizadas correctamente en carpetas 'train' y 'test' sin fugas de datos.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REVISAR GENERADORES!!!!"
      ],
      "metadata": {
        "id": "8UJJGQDxS4Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "def crear_generadores(carpeta_destino_base, batch_size=128):\n",
        "  train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  # Generador de datos para validación\n",
        "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "  # Crear generador para entrenamiento\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "      os.path.join(carpeta_destino_base, 'train'),\n",
        "      target_size=(224, 224),\n",
        "      batch_size=batch_size, #32\n",
        "      class_mode='sparse', # sparse_categorical_crossentropy\n",
        "      shuffle=True\n",
        "      )\n",
        "\n",
        "  # Crear generador para validación\n",
        "  validation_generator = test_datagen.flow_from_directory(\n",
        "      os.path.join(carpeta_destino_base, 'test'),\n",
        "      target_size=(224, 224),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='sparse', # sparse_categorical_crossentropy\n",
        "      shuffle=False # Correcto para validación\n",
        "      )\n",
        "\n",
        "  print(\"Generadores de datos creados.\")\n",
        "  return train_generator, validation_generator\n",
        "\n",
        "train_generator, validation_generator = crear_generadores('/content/dataset')"
      ],
      "metadata": {
        "id": "fTh-YbmSS0_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6d82d5-10fc-4bb1-d72e-6bb7a4f554d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11690 images belonging to 2 classes.\n",
            "Found 5012 images belonging to 2 classes.\n",
            "Generadores de datos creados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_v1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), #\n",
        "    loss='sparse_categorical_crossentropy', # O 'categorical' si usas one-hot encoding\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "Rui1u4CuX3h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "csv_logger_TL = CSVLogger('log_model_TL.csv', separator=',', append=False)"
      ],
      "metadata": {
        "id": "FkKllKv2vwnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks_TL = [\n",
        "    csv_logger_TL, # Logger\n",
        "]\n",
        "\n",
        "print(\"\\n--- Iniciando Entrenamiento (Transfer Learning) ---\")\n",
        "\n",
        "history = model_v1.fit(\n",
        "    train_generator,\n",
        "    epochs=25, #Según Paper\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks_TL\n",
        ")\n",
        "\n",
        "print(\"--- Entrenamiento (Transfer Learning) Finalizado ---\")"
      ],
      "metadata": {
        "id": "SOWK6wzlvCzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0218999-897f-4890-c6c9-00e80f7d0851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iniciando Entrenamiento (Transfer Learning) ---\n",
            "Epoch 1/25\n",
            "366/366 [==============================] - 240s 634ms/step - loss: 0.7542 - accuracy: 0.5395 - val_loss: 0.6353 - val_accuracy: 0.6443\n",
            "Epoch 2/25\n",
            "366/366 [==============================] - 226s 617ms/step - loss: 0.6809 - accuracy: 0.6133 - val_loss: 0.6129 - val_accuracy: 0.6706\n",
            "Epoch 3/25\n",
            "366/366 [==============================] - 226s 618ms/step - loss: 0.6458 - accuracy: 0.6405 - val_loss: 0.5908 - val_accuracy: 0.6866\n",
            "Epoch 4/25\n",
            "366/366 [==============================] - 232s 634ms/step - loss: 0.6365 - accuracy: 0.6507 - val_loss: 0.5856 - val_accuracy: 0.6923\n",
            "Epoch 5/25\n",
            "366/366 [==============================] - 233s 638ms/step - loss: 0.6253 - accuracy: 0.6631 - val_loss: 0.5789 - val_accuracy: 0.7005\n",
            "Epoch 6/25\n",
            "366/366 [==============================] - 235s 641ms/step - loss: 0.6163 - accuracy: 0.6682 - val_loss: 0.5761 - val_accuracy: 0.7019\n",
            "Epoch 7/25\n",
            "366/366 [==============================] - 224s 612ms/step - loss: 0.6113 - accuracy: 0.6743 - val_loss: 0.5724 - val_accuracy: 0.7041\n",
            "Epoch 8/25\n",
            "366/366 [==============================] - 227s 619ms/step - loss: 0.6026 - accuracy: 0.6814 - val_loss: 0.5732 - val_accuracy: 0.7041\n",
            "Epoch 9/25\n",
            "366/366 [==============================] - 229s 625ms/step - loss: 0.5967 - accuracy: 0.6858 - val_loss: 0.5660 - val_accuracy: 0.7101\n",
            "Epoch 10/25\n",
            "366/366 [==============================] - 228s 623ms/step - loss: 0.5958 - accuracy: 0.6867 - val_loss: 0.5639 - val_accuracy: 0.7103\n",
            "Epoch 11/25\n",
            "366/366 [==============================] - 227s 619ms/step - loss: 0.5907 - accuracy: 0.6896 - val_loss: 0.5636 - val_accuracy: 0.7117\n",
            "Epoch 12/25\n",
            "366/366 [==============================] - 226s 617ms/step - loss: 0.5920 - accuracy: 0.6906 - val_loss: 0.5610 - val_accuracy: 0.7115\n",
            "Epoch 13/25\n",
            "366/366 [==============================] - 226s 618ms/step - loss: 0.5907 - accuracy: 0.6925 - val_loss: 0.5620 - val_accuracy: 0.7113\n",
            "Epoch 14/25\n",
            "366/366 [==============================] - 229s 625ms/step - loss: 0.5834 - accuracy: 0.6968 - val_loss: 0.5597 - val_accuracy: 0.7143\n",
            "Epoch 15/25\n",
            "366/366 [==============================] - 226s 618ms/step - loss: 0.5880 - accuracy: 0.6919 - val_loss: 0.5595 - val_accuracy: 0.7163\n",
            "Epoch 16/25\n",
            "366/366 [==============================] - 234s 638ms/step - loss: 0.5823 - accuracy: 0.6984 - val_loss: 0.5601 - val_accuracy: 0.7147\n",
            "Epoch 17/25\n",
            "366/366 [==============================] - 234s 641ms/step - loss: 0.5815 - accuracy: 0.6999 - val_loss: 0.5568 - val_accuracy: 0.7139\n",
            "Epoch 18/25\n",
            "366/366 [==============================] - 236s 644ms/step - loss: 0.5788 - accuracy: 0.6999 - val_loss: 0.5608 - val_accuracy: 0.7145\n",
            "Epoch 19/25\n",
            "366/366 [==============================] - 232s 634ms/step - loss: 0.5791 - accuracy: 0.7010 - val_loss: 0.5558 - val_accuracy: 0.7153\n",
            "Epoch 20/25\n",
            "366/366 [==============================] - 234s 639ms/step - loss: 0.5790 - accuracy: 0.7015 - val_loss: 0.5556 - val_accuracy: 0.7161\n",
            "Epoch 21/25\n",
            "366/366 [==============================] - 238s 649ms/step - loss: 0.5739 - accuracy: 0.7007 - val_loss: 0.5581 - val_accuracy: 0.7179\n",
            "Epoch 22/25\n",
            "366/366 [==============================] - 232s 635ms/step - loss: 0.5753 - accuracy: 0.7021 - val_loss: 0.5546 - val_accuracy: 0.7231\n",
            "Epoch 23/25\n",
            "366/366 [==============================] - 233s 636ms/step - loss: 0.5791 - accuracy: 0.6970 - val_loss: 0.5541 - val_accuracy: 0.7195\n",
            "Epoch 24/25\n",
            "366/366 [==============================] - 232s 634ms/step - loss: 0.5743 - accuracy: 0.7023 - val_loss: 0.5532 - val_accuracy: 0.7225\n",
            "Epoch 25/25\n",
            "366/366 [==============================] - 233s 636ms/step - loss: 0.5757 - accuracy: 0.7000 - val_loss: 0.5548 - val_accuracy: 0.7185\n",
            "--- Entrenamiento (Transfer Learning) Finalizado ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el estado final absoluto\n",
        "model_v1.save('final_googlenet_baseline.keras')\n",
        "print(\"Modelo final guardado.\")\n",
        "from google.colab import files\n",
        "\n",
        "# Nombre del archivo que definimos en el checkpoint\n",
        "filename = 'final_googlenet_baseline.keras'\n",
        "\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "GugZYFD1xS6i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f62e2dc3-2fdc-4830-8f4e-b2d1b7f665e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo final guardado.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2fbc885d-f6e3-431d-9498-4e9bdab3d234\", \"final_googlenet_baseline.keras\", 22560223)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Crear la carpeta para guardar los modelos si no existe\n",
        "os.makedirs('/content/MisModelos', exist_ok=True)\n",
        "model_v1.save('/content/MisModelos/model_v1.h5')\n",
        "print(\"Modelos guardados exitosamente en /content/MisModelos/\")"
      ],
      "metadata": {
        "id": "U9P74huzwYne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56771a9-5e63-4ce7-ac01-d31423ce42a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos guardados exitosamente en /content/MisModelos/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Ruta del modelo específico que se guardó en la celda anterior\n",
        "modelo_path = '/content/MisModelos/model_v1.h5'\n",
        "\n",
        "print(f\"Intentando descargar el modelo: {os.path.basename(modelo_path)}\")\n",
        "\n",
        "# Verificar si el archivo del modelo existe antes de iniciar la descarga\n",
        "if os.path.exists(modelo_path):\n",
        "    files.download(modelo_path)\n",
        "    print(\"\\nDescarga completada exitosamente.\")\n",
        "else:\n",
        "    print(f\"\\nError: No se encontró el archivo del modelo en la ruta especificada: {modelo_path}\")\n",
        "    print(\"Por favor, asegúrate de que la celda anterior para guardar el modelo se haya ejecutado sin errores.\")"
      ],
      "metadata": {
        "id": "5glrFL7VTBPf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6c80e20a-c1cc-4277-bafd-7fd46cf3e429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intentando descargar el modelo: model_v1.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79090031-1023-4c27-ac7f-5697d3c35f4b\", \"model_v1.h5\", 22740288)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descarga completada exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def convertir_csv_a_excel(ruta_csv, ruta_excel):\n",
        "\n",
        "    if not os.path.exists(ruta_csv):\n",
        "        print(f\"Advertencia: El archivo CSV no fue encontrado en la ruta: {ruta_csv}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Leer el archivo CSV con pandas\n",
        "        df = pd.read_csv(ruta_csv)\n",
        "\n",
        "        # Guardar el DataFrame como archivo Excel\n",
        "        df.to_excel(ruta_excel, index=False)\n",
        "\n",
        "        print(f\"Métricas de '{ruta_csv}' guardadas exitosamente en: {ruta_excel}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante la conversión de '{ruta_csv}' a Excel: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "Ziwl2oqWTGBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Lista de los archivos CSV de registro que se generaron\n",
        "archivos_csv_registros = [\n",
        "    'log_model_TL.csv'\n",
        "]\n",
        "\n",
        "# Carpeta para guardar los archivos Excel\n",
        "carpeta_excel = '/content/Metricas_Excel/'\n",
        "os.makedirs(carpeta_excel, exist_ok=True)\n",
        "\n",
        "print(\"--- Iniciando conversión y descarga de métricas ---\")\n",
        "for nombre_csv in archivos_csv_registros:\n",
        "    nombre_excel = nombre_csv.replace('.csv', '.xlsx')\n",
        "    ruta_excel_destino = os.path.join(carpeta_excel, nombre_excel)\n",
        "\n",
        "    # Intentar convertir el archivo\n",
        "    if convertir_csv_a_excel(nombre_csv, ruta_excel_destino):\n",
        "        # Si la conversión fue exitosa, descargar el archivo\n",
        "        print(f\"Iniciando descarga de {ruta_excel_destino}...\")\n",
        "        files.download(ruta_excel_destino)\n",
        "\n",
        "print(\"\\n--- Proceso de conversión y descarga finalizado. ---\")"
      ],
      "metadata": {
        "id": "AppyYgpHwrgb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a706ba0e-1360-4b01-983a-e3f42bd97bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando conversión y descarga de métricas ---\n",
            "Métricas de 'log_model_TL.csv' guardadas exitosamente en: /content/Metricas_Excel/log_model_TL.xlsx\n",
            "Iniciando descarga de /content/Metricas_Excel/log_model_TL.xlsx...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_89665c4a-dfaa-4c5a-b993-221ac8235454\", \"log_model_TL.xlsx\", 6491)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Proceso de conversión y descarga finalizado. ---\n"
          ]
        }
      ]
    }
  ]
}